{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to do to improve performance.\n",
    "\n",
    "So the first submission didn't score as well as the train set, not surprising!\n",
    "\n",
    "Lets try and;\n",
    "\n",
    "1. Extract more of the text from the html, the body and title aren't enough it seems.\n",
    "2. Over/undersample and see if I improve performance with either strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jake\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jake\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jake\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tldextract\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "data_dir = \"../data/2018-08-10_AV_Innoplexus/\"\n",
    "\n",
    "#After we use get_text, use nltk's clean_html function.\n",
    "def nltkPipe(soup_text):\n",
    "    #Convert to tokens\n",
    "    tokens = [x.lower() for x in wordpunct_tokenize(soup_text)]\n",
    "    text = nltk.Text(tokens)\n",
    "    #Get lowercase words. No single letters, and no stop words\n",
    "    words = [w.lower() for w in text if w.isalpha() and len(w) > 1 and w.lower() not in stop_words]\n",
    "    #Remove prefix/suffixes to cut down on vocab\n",
    "    stemmer = EnglishStemmer()\n",
    "    words_nostems = [stemmer.stem(w) for w in words]\n",
    "    return ', '.join(words_nostems)\n",
    "\n",
    "def getTitleTokens(soup):\n",
    "    soup_title = soup.title\n",
    "    if soup_title != None:\n",
    "        soup_title_text = soup.title.get_text()\n",
    "        text_arr = nltkPipe(soup_title_text)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def getBodyTokens(soup):\n",
    "    #Get the text body\n",
    "    soup_para = soup.find_all('p')\n",
    "    if soup_para != None:\n",
    "        soup_para_clean = ' '.join([x.get_text() for x in soup_para if x.span==None and x.a==None])\n",
    "        text_arr = nltkPipe(soup_para_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def getDomainTokens(domainstr):\n",
    "    domain_extracted = tldextract.extract(domainstr)#.domain\n",
    "    domain_tokens = nltkPipe(domain_extracted.domain+\",\"+domain_extracted.suffix)\n",
    "    return domain_tokens\n",
    "\n",
    "def getUrlTokens(url):\n",
    "    domain_split = url.rsplit('/')\n",
    "    if len(domain_split) > 1:\n",
    "        domain_split_elements = ' '.join(domain_split[1:])\n",
    "        domain_split_tokens = nltkPipe(domain_split_elements)\n",
    "        return domain_split_tokens\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def getDescriptionTokens(soup):\n",
    "    #Get the text body\n",
    "    soup_desc = soup.find_all('dl')\n",
    "    if soup_desc != None:\n",
    "        soup_desc_clean = ' '.join([x.get_text() for x in soup_desc])\n",
    "        text_arr = nltkPipe(soup_desc_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def getHeaderTokens(soup):\n",
    "    #Get the html header tokens\n",
    "    soup_heads = soup.find_all('header')\n",
    "    if soup_heads != None:\n",
    "        soup_heads_clean = ' '.join([x.get_text() for x in soup_heads])\n",
    "        text_arr = nltkPipe(soup_heads_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def getHeadTokens(soup):\n",
    "    #Get the html head tokens\n",
    "    soup_head = soup.find_all('head')\n",
    "    if soup_head != None:\n",
    "        soup_head_clean = ' '.join([x.get_text() for x in soup_head])\n",
    "        text_arr = nltkPipe(soup_head_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def getFontTokens(soup):\n",
    "    soup_font = soup.find_all('font')\n",
    "    if soup_font != None:\n",
    "        soup_font_clean = ' '.join([x.get_text() for x in soup_font])\n",
    "        text_arr = nltkPipe(soup_font_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def getTableTokens(soup):\n",
    "    soup_table = soup.find_all('table')\n",
    "    soup_table_headers = [[a for a in x.find_all('th')] for x in soup_table]\n",
    "    soup_table_cells = [[a for a in x.find_all('td')] for x in soup_table]\n",
    "    if soup_table != None:\n",
    "        soup_table_headers_clean = ' '.join([' '.join([a.get_text() for a in x]) for x in soup_table_headers])\n",
    "        soup_table_cells_clean = ' '.join([' '.join([a.get_text() for a in x]) for x in soup_table_cells])\n",
    "        text_arr = nltkPipe(soup_table_headers_clean+soup_table_cells_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def getHrefTokens(soup):\n",
    "    soup_href = soup.find_all('href')\n",
    "    if soup_href != None:\n",
    "        soup_href_clean = ' '.join([x.get_text() for x in soup_href])\n",
    "        text_arr = nltkPipe(soup_href_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def getListTokens(soup):\n",
    "    soup_list = soup.find_all('ol') + soup.find_all('ul')\n",
    "    if soup_list != None:\n",
    "        soup_list_items = [x.find_all('li') for x in soup_list]\n",
    "        soup_list_items_clean = ' '.join([' '.join([a.get_text() for a in x]) for x in soup_list_items])\n",
    "        text_arr = nltkPipe(soup_list_items_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def get_all_tokens(frame):\n",
    "    print(\"Parsing domain tokens...\")\n",
    "    domain_tokens = frame['Domain'].apply(getDomainTokens)\n",
    "    print(\"Parsing url tokens...\")\n",
    "    url_tokens = frame['Url'].apply(getUrlTokens)\n",
    "    print(\"Parsing soup...\")\n",
    "    soup = frame['Html'].apply(lambda x: BeautifulSoup(x, 'html.parser'))\n",
    "    print(\"Getting title tokens...\")\n",
    "    title_tokens = soup.apply(getTitleTokens)\n",
    "    print(\"Getting body tokens...\")\n",
    "    body_tokens = soup.apply(getBodyTokens)\n",
    "    print(\"Getting description tokens...\")\n",
    "    description_tokens = soup.apply(getDescriptionTokens)\n",
    "    print(\"Getting header tokens...\")\n",
    "    header_tokens = soup.apply(getHeaderTokens)\n",
    "    print(\"Getting head-metadata tokens...\")\n",
    "    head_tokens = soup.apply(getHeadTokens)\n",
    "    print(\"Getting font tokens...\")\n",
    "    font_tokens = soup.apply(getFontTokens)\n",
    "    print(\"Getting table tokens...\")\n",
    "    table_tokens = soup.apply(getTableTokens)\n",
    "    print(\"Getting href tokens...\")\n",
    "    href_tokens = soup.apply(getHrefTokens)\n",
    "    print(\"Getting list tokens...\")\n",
    "    list_tokens = soup.apply(getListTokens)\n",
    "    print(\"Done!\")\n",
    "    return title_tokens + body_tokens + domain_tokens + url_tokens\\\n",
    "    + description_tokens + header_tokens + head_tokens + font_tokens +\\\n",
    "    table_tokens + href_tokens + list_tokens\n",
    "\n",
    "#Build the model\n",
    "def get_html(in_df, out_file_name, chunk_size=5000, overwrite=False, test=False):\n",
    "    keep_cols = [\"Webpage_id\",\"Domain\",\"Url\",\"Tag\"]\n",
    "    read_cols = [\"Webpage_id\",\"all_tokens\",\"Tag\"]\n",
    "    if test:\n",
    "        keep_cols = [\"Webpage_id\",\"Domain\",\"Url\"]\n",
    "        read_cols = [\"Webpage_id\",\"all_tokens\"]\n",
    "    if os.path.isfile(data_dir+out_file_name)==False:\n",
    "        if test:\n",
    "            out_frame = pd.DataFrame(columns=[\"Webpage_id\",\"all_tokens\"])\n",
    "        else:\n",
    "            out_frame = pd.DataFrame(columns=[\"Webpage_id\",\"Tag\",\"all_tokens\"])\n",
    "        out_frame.to_csv(data_dir+out_file_name,index=False)\n",
    "    else:\n",
    "        if overwrite:\n",
    "            if test:\n",
    "                out_frame = pd.DataFrame(columns=[\"Webpage_id\",\"all_tokens\"])\n",
    "            else:\n",
    "                out_frame = pd.DataFrame(columns=[\"Webpage_id\",\"Tag\",\"all_tokens\"])\n",
    "            out_frame.to_csv(data_dir+out_file_name,index=False)\n",
    "    use_df = in_df[keep_cols]\n",
    "    html_reader_obj = pd.read_csv(data_dir+'html_data.csv',iterator=True, chunksize=chunk_size)\n",
    "    match_indices = use_df['Webpage_id'].values.tolist()\n",
    "    print(\"Getting tokens...\")\n",
    "    print(len(match_indices),' indices left...')\n",
    "    while len(match_indices) > 0:\n",
    "        for chunk in html_reader_obj:\n",
    "            merge_df = pd.merge(use_df,chunk,how='inner',on='Webpage_id')\n",
    "            merge_df['all_tokens'] = get_all_tokens(merge_df)\n",
    "            merge_df.drop(['Html','Domain','Url'],axis=1,inplace=True)\n",
    "            merge_indices = merge_df['Webpage_id'].values.tolist()\n",
    "            match_indices = [x for x in match_indices if x not in merge_indices]\n",
    "            print(len(match_indices),' indices left...')\n",
    "            concat_frame = pd.read_csv(data_dir+out_file_name,usecols=read_cols)\n",
    "            return_frame = concat_frame.append(merge_df)[read_cols]\n",
    "            return_frame.to_csv(data_dir+out_file_name,index=False)\n",
    "            #frames.append(merge_df)\n",
    "    #Process HTMl for bags of words of the body and title.\n",
    "    #process_df = pd.concat(frames)\n",
    "    print(\"Done! You can get your file at\\n\"+data_dir+out_file_name)\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"Return the estimator and the object to transform the test data.\"\"\"    \n",
    "    train_df = pd.read_csv(data_dir+'train.csv')\n",
    "    tags = train_df['Tag']\n",
    "    #Get tokens\n",
    "    train_df = get_html(train_df)\n",
    "    #Fit_transform to tdfif matrix\n",
    "    print(\"Transforming to tdfif_matrix...\")\n",
    "    train_df = vectorizer.fit_transform(train_df['all_tokens'])\n",
    "    #Prune unneeded features\n",
    "    print(\"Performing SVD...\")\n",
    "    train_df = svd.fit_transform(train_df)\n",
    "    \n",
    "    vector_features = vectorizer.get_feature_names()\n",
    "    eigen_features = [vector_features[i] for i in svd.components_[0].argsort()[::-1]][:500]\n",
    "\n",
    "    train_df = pd.DataFrame(train_df,columns=eigen_features)\n",
    "    train_df['Tag'] = tags\n",
    "    \n",
    "    tags = train_df['Tag'].unique().tolist()\n",
    "    tags.sort()\n",
    "\n",
    "    tag_dict = {key: value for (key, value) in zip(tags,range(len(tags)))}\n",
    "\n",
    "    train_df['Tag_encoded'] = train_df['Tag'].map(tag_dict)\n",
    "    train_df = train_df.drop('Tag',axis=1)\n",
    "    #Build the model\n",
    "    print(\"Building the model...\")\n",
    "    exported_pipeline = make_pipeline(\n",
    "        StackingEstimator(\n",
    "            estimator=ExtraTreesClassifier(\n",
    "                bootstrap=False, criterion=\"gini\", max_features=0.2, \n",
    "                min_samples_leaf=11, min_samples_split=17, n_estimators=100)\n",
    "        ),\n",
    "        ExtraTreesClassifier(\n",
    "            bootstrap=False, criterion=\"entropy\", max_features=0.5, \n",
    "            min_samples_leaf=6, min_samples_split=9, n_estimators=100\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    x_cols = [x for x in train_df_svd.columns if x != \"Tag_encoded\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        train_df[x_cols],\n",
    "        train_df['Tag_encoded'],\n",
    "        test_size=0.33\n",
    "    )\n",
    "    print(\"Fitting the model...\")\n",
    "    exported_pipeline.fit(X_train, y_train)\n",
    "    print(\"Done!\")\n",
    "    return exported_pipeline, vectorizer, svd, tag_dict\n",
    "\n",
    "def prep_test(vectorizer_obj, svd_obj):\n",
    "    \"\"\"Transform test dataset for predicting.\"\"\"\n",
    "    print(\"Getting tokens from html...\")\n",
    "    test_df = pd.read_csv(data_dir+'test.csv')\n",
    "    #Get the HTMl\n",
    "    test_df_tokens = get_html(test_df)\n",
    "    #Transform to tdfif matrix\n",
    "    print(\"Transforming to tfidf matrix...\")\n",
    "    test_df_tdif = vectorizer_obj.transform(test_df_tokens['all_tokens'])\n",
    "    #Prune unneeded features\n",
    "    print(\"Performing SVD...\")\n",
    "    test_svd_array = svd_obj.transform(test_df_tdif)\n",
    "    \n",
    "    vector_features = vectorizer_obj.get_feature_names()\n",
    "    eigen_features = [vector_features[i] for i in svd_obj.components_[0].argsort()[::-1]][:500]\n",
    "    #Map to dataframe\n",
    "    test_df_svd = pd.DataFrame(test_svd_array,columns=eigen_features)\n",
    "    test_df_svd['Tag'] = test_df['Tag']\n",
    "    print(\"Done!\")\n",
    "    return test_df_svd\n",
    "\n",
    "def main():\n",
    "    #Get the model\n",
    "    print(\"Getting the model, transform objects and tag-dict...\")\n",
    "    model, vectorizer_obj, svd_obj, tag_dict = build_model()\n",
    "    #Prep the test set\n",
    "    print(\"Prepping the test dataset...\")\n",
    "    test_df = prep_test(vectorizer_obj, svd_obj)\n",
    "    print(\"Making predictions...\")\n",
    "    predictions = model.predict(test_df)\n",
    "    print(\"Formatting predictions...\")\n",
    "    print(\"Saving predictions for submission...\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try and extract more tokens. I want to extract tokens from; description, headings, highlights, special fonts, table and list elements.\n",
    "\n",
    "To do this I'll write an html reading script for a subset of rows to test my parser on. Then I'll incorporate into the get_html function and make sure I get all tokens to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Webpage_id</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Url</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47212</th>\n",
       "      <td>70113</td>\n",
       "      <td>www.medbox.org</td>\n",
       "      <td>https://www.medbox.org/clinical-guidelines/mal...</td>\n",
       "      <td>guidelines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5860</th>\n",
       "      <td>8570</td>\n",
       "      <td>www.dart-europe.eu</td>\n",
       "      <td>http://www.dart-europe.eu/full.php?id=110600</td>\n",
       "      <td>thesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30825</th>\n",
       "      <td>46028</td>\n",
       "      <td>www.ivfforums.gr</td>\n",
       "      <td>http://www.ivfforums.gr/ivf-moms-magazine/cook...</td>\n",
       "      <td>forum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33088</th>\n",
       "      <td>49622</td>\n",
       "      <td>www.childrenshospital.org</td>\n",
       "      <td>http://www.childrenshospital.org/news-and-even...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48957</th>\n",
       "      <td>72270</td>\n",
       "      <td>curate.nd.edu</td>\n",
       "      <td>https://curate.nd.edu/show/1c18df67n1c</td>\n",
       "      <td>thesis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Webpage_id                     Domain  \\\n",
       "47212       70113             www.medbox.org   \n",
       "5860         8570         www.dart-europe.eu   \n",
       "30825       46028           www.ivfforums.gr   \n",
       "33088       49622  www.childrenshospital.org   \n",
       "48957       72270              curate.nd.edu   \n",
       "\n",
       "                                                     Url         Tag  \n",
       "47212  https://www.medbox.org/clinical-guidelines/mal...  guidelines  \n",
       "5860        http://www.dart-europe.eu/full.php?id=110600      thesis  \n",
       "30825  http://www.ivfforums.gr/ivf-moms-magazine/cook...       forum  \n",
       "33088  http://www.childrenshospital.org/news-and-even...        news  \n",
       "48957             https://curate.nd.edu/show/1c18df67n1c      thesis  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(data_dir+'train.csv')\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Webpage_id</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5012</th>\n",
       "      <td>15530</td>\n",
       "      <td>tlcr.amegroups.com</td>\n",
       "      <td>http://tlcr.amegroups.com/article/view/3192/3746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19299</th>\n",
       "      <td>58628</td>\n",
       "      <td>go.qiagen.com</td>\n",
       "      <td>https://go.qiagen.com/ngs-assay-menu?elq=00000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14266</th>\n",
       "      <td>43055</td>\n",
       "      <td>www.mhi.interv.org</td>\n",
       "      <td>http://www.mhi.interv.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7810</th>\n",
       "      <td>23543</td>\n",
       "      <td>cshprotocols.cshlp.org</td>\n",
       "      <td>http://cshprotocols.cshlp.org/content/2014/3/p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4420</th>\n",
       "      <td>13985</td>\n",
       "      <td>www.mims.co.uk</td>\n",
       "      <td>http://www.mims.co.uk/risk-abuse-gabapentin-pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Webpage_id                  Domain  \\\n",
       "5012        15530      tlcr.amegroups.com   \n",
       "19299       58628           go.qiagen.com   \n",
       "14266       43055      www.mhi.interv.org   \n",
       "7810        23543  cshprotocols.cshlp.org   \n",
       "4420        13985          www.mims.co.uk   \n",
       "\n",
       "                                                     Url  \n",
       "5012    http://tlcr.amegroups.com/article/view/3192/3746  \n",
       "19299  https://go.qiagen.com/ngs-assay-menu?elq=00000...  \n",
       "14266                         http://www.mhi.interv.org/  \n",
       "7810   http://cshprotocols.cshlp.org/content/2014/3/p...  \n",
       "4420   http://www.mims.co.uk/risk-abuse-gabapentin-pr...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(data_dir+'test.csv')\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tokens...\n",
      "53447  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "50065  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "46616  indices left...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jake\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "43146  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "40091  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "36762  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "33466  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "30229  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "26910  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "23473  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "20084  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "16761  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "13419  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "9967  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "6322  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "2857  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "0  indices left...\n",
      "Done! You can get your file at\n",
      "../data/2018-08-10_AV_Innoplexus/train_df_all_tokens.csv\n"
     ]
    }
   ],
   "source": [
    "get_html(train_df,'train_df_all_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tokens...\n",
      "25787  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "24169  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "22618  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "21088  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "19143  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "17472  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "15768  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "14005  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "12325  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "10778  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "9169  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "7555  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "5913  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "4367  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "3023  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "1488  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "0  indices left...\n",
      "Done! You can get your file at\n",
      "../data/2018-08-10_AV_Innoplexus/test_df_all_tokens.csv\n"
     ]
    }
   ],
   "source": [
    "get_html(test_df,'test_df_all_tokens.csv', test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
