{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jdber\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jdber\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tldextract\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "data_dir = \"../data/2018-08-10_AV_Innoplexus/\"\n",
    "\n",
    "#After we use get_text, use nltk's clean_html function.\n",
    "def nltkPipe(soup_text):\n",
    "    #Convert to tokens\n",
    "    tokens = [x.lower() for x in wordpunct_tokenize(soup_text)]\n",
    "    text = nltk.Text(tokens)\n",
    "    #Get lowercase words. No single letters, and no stop words\n",
    "    words = [w.lower() for w in text if w.isalpha() and len(w) > 1 and w.lower() not in stop_words]\n",
    "    #Remove prefix/suffixes to cut down on vocab\n",
    "    stemmer = EnglishStemmer()\n",
    "    words_nostems = [stemmer.stem(w) for w in words]\n",
    "    return ', '.join(words_nostems)\n",
    "\n",
    "def getTitleTokens(soup):\n",
    "    soup_title = soup.title\n",
    "    if soup_title != None:\n",
    "        soup_title_text = soup.title.get_text()\n",
    "        text_arr = nltkPipe(soup_title_text)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def getBodyTokens(soup):\n",
    "    #Get the text body\n",
    "    soup_para = soup.find_all('p')\n",
    "    soup_para_clean = ' '.join([x.get_text() for x in soup_para if x.span==None and x.a==None])\n",
    "    text_arr = nltkPipe(soup_para_clean)\n",
    "    return text_arr\n",
    "\n",
    "def getDomainTokens(domainstr):\n",
    "    domain_extracted = tldextract.extract(domainstr)#.domain\n",
    "    domain_tokens = nltkPipe(domain_extracted.domain+\",\"+domain_extracted.suffix)\n",
    "    return domain_tokens\n",
    "\n",
    "def get_all_tokens(frame):\n",
    "    print(\"Parsing domain tokens...\")\n",
    "    domain_tokens = frame['Domain'].apply(getDomainTokens)\n",
    "    print(\"Parsing soup...\")\n",
    "    soup = frame['Html'].apply(lambda x: BeautifulSoup(x, 'html.parser'))\n",
    "    print(\"Getting title tokens...\")\n",
    "    title_tokens = soup.apply(getTitleTokens)\n",
    "    print(\"Getting body tokens...\")\n",
    "    body_tokens = soup.apply(getBodyTokens)\n",
    "    print(\"Done!\")\n",
    "    return title_tokens + body_tokens + domain_tokens\n",
    "\n",
    "#Build the model\n",
    "def get_html(in_df, test=False):\n",
    "    keep_cols = [\"Webpage_id\",\"Tag\",\"Domain\"]\n",
    "    if test:\n",
    "        keep_cols = [\"Webpage_id\",\"Domain\"]\n",
    "    use_df = in_df[keep_cols]\n",
    "    html_reader_obj = pd.read_csv(data_dir+'html_data.csv',iterator=True, chunksize=10000)\n",
    "    frames = []\n",
    "    match_indices = use_df['Webpage_id'].values.tolist()\n",
    "    print(\"Getting tokens...\")\n",
    "    print(len(match_indices),' indices left...')\n",
    "    while len(match_indices) > 0:\n",
    "        for chunk in html_reader_obj:\n",
    "            merge_df = pd.merge(use_df,chunk,how='inner',on='Webpage_id')\n",
    "            merge_df['all_tokens'] = get_all_tokens(merge_df)\n",
    "            merge_df.drop(['Html','Domain'],axis=1,inplace=True)\n",
    "            merge_indices = merge_df['Webpage_id'].values.tolist()\n",
    "            match_indices = [x for x in match_indices if x not in merge_indices]\n",
    "            print(len(match_indices),' indices left...')\n",
    "            frames.append(merge_df)\n",
    "    #Process HTMl for bags of words of the body and title.\n",
    "    process_df = pd.concat(frames)\n",
    "    print(\"Done!\")\n",
    "    return process_df\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"Return the estimator and the object to transform the test data.\"\"\"    \n",
    "    train_df = pd.read_csv(data_dir+'train.csv')\n",
    "    tags = train_df['Tag']\n",
    "    #Get tokens\n",
    "    train_df = get_html(train_df)\n",
    "    #Fit_transform to tdfif matrix\n",
    "    print(\"Transforming to tdfif_matrix...\")\n",
    "    train_df = vectorizer.fit_transform(train_df['all_tokens'])\n",
    "    #Prune unneeded features\n",
    "    print(\"Performing SVD...\")\n",
    "    train_df = svd.fit_transform(train_df)\n",
    "    \n",
    "    vector_features = vectorizer.get_feature_names()\n",
    "    eigen_features = [vector_features[i] for i in svd.components_[0].argsort()[::-1]][:500]\n",
    "\n",
    "    train_df = pd.DataFrame(train_df,columns=eigen_features)\n",
    "    train_df['Tag'] = tags\n",
    "    \n",
    "    tags = train_df['Tag'].unique().tolist()\n",
    "    tags.sort()\n",
    "\n",
    "    tag_dict = {key: value for (key, value) in zip(tags,range(len(tags)))}\n",
    "\n",
    "    train_df['Tag_encoded'] = train_df['Tag'].map(tag_dict)\n",
    "    train_df = train_df.drop('Tag',axis=1)\n",
    "    #Build the model\n",
    "    print(\"Building the model...\")\n",
    "    exported_pipeline = make_pipeline(\n",
    "        StackingEstimator(\n",
    "            estimator=ExtraTreesClassifier(\n",
    "                bootstrap=False, criterion=\"gini\", max_features=0.2, \n",
    "                min_samples_leaf=11, min_samples_split=17, n_estimators=100)\n",
    "        ),\n",
    "        ExtraTreesClassifier(\n",
    "            bootstrap=False, criterion=\"entropy\", max_features=0.5, \n",
    "            min_samples_leaf=6, min_samples_split=9, n_estimators=100\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    x_cols = [x for x in train_df_svd.columns if x != \"Tag_encoded\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        train_df[x_cols],\n",
    "        train_df['Tag_encoded'],\n",
    "        test_size=0.33\n",
    "    )\n",
    "    print(\"Fitting the model...\")\n",
    "    exported_pipeline.fit(X_train, y_train)\n",
    "    print(\"Done!\")\n",
    "    return exported_pipeline, vectorizer, svd, tag_dict\n",
    "\n",
    "def prep_test(vectorizer_obj, svd_obj):\n",
    "    \"\"\"Transform test dataset for predicting.\"\"\"\n",
    "    print(\"Getting tokens from html...\")\n",
    "    test_df = pd.read_csv(data_dir+'test.csv')\n",
    "    #Get the HTMl\n",
    "    test_df_tokens = get_html(test_df)\n",
    "    #Transform to tdfif matrix\n",
    "    print(\"Transforming to tfidf matrix...\")\n",
    "    test_df_tdif = vectorizer_obj.transform(test_df_tokens['all_tokens'])\n",
    "    #Prune unneeded features\n",
    "    print(\"Performing SVD...\")\n",
    "    test_svd_array = svd_obj.transform(test_df_tdif)\n",
    "    \n",
    "    vector_features = vectorizer_obj.get_feature_names()\n",
    "    eigen_features = [vector_features[i] for i in svd_obj.components_[0].argsort()[::-1]][:500]\n",
    "    #Map to dataframe\n",
    "    test_df_svd = pd.DataFrame(test_svd_array,columns=eigen_features)\n",
    "    test_df_svd['Tag'] = test_df['Tag']\n",
    "    print(\"Done!\")\n",
    "    return test_df_svd\n",
    "\n",
    "def main():\n",
    "    #Get the model\n",
    "    print(\"Getting the model, transform objects and tag-dict...\")\n",
    "    model, vectorizer_obj, svd_obj, tag_dict = build_model()\n",
    "    #Prep the test set\n",
    "    print(\"Prepping the test dataset...\")\n",
    "    test_df = prep_test(vectorizer_obj, svd_obj)\n",
    "    print(\"Making predictions...\")\n",
    "    predictions = model.predict(test_df)\n",
    "    print(\"Formatting predictions...\")\n",
    "    print(\"Saving predictions for submission...\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Webpage_id</th>\n",
       "      <th>Tag</th>\n",
       "      <th>all_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16427</th>\n",
       "      <td>24663</td>\n",
       "      <td>news</td>\n",
       "      <td>'allogen', 'transplant', 'age', 'still', 'matt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19326</th>\n",
       "      <td>29109</td>\n",
       "      <td>others</td>\n",
       "      <td>'yorkshir', 'even', 'post', 'obituari', 'yorks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24157</th>\n",
       "      <td>36422</td>\n",
       "      <td>profile</td>\n",
       "      <td>'bruce', 'faddegon', 'phd', 'ucsf', 'helen', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25974</th>\n",
       "      <td>39212</td>\n",
       "      <td>profile</td>\n",
       "      <td>'mit', 'nse', 'faculti', 'neil', 'todrea''kepc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35900</th>\n",
       "      <td>53763</td>\n",
       "      <td>others</td>\n",
       "      <td>'exrna', 'home''advertis', 'exrna', 'accept', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Webpage_id      Tag                                         all_tokens\n",
       "16427       24663     news  'allogen', 'transplant', 'age', 'still', 'matt...\n",
       "19326       29109   others  'yorkshir', 'even', 'post', 'obituari', 'yorks...\n",
       "24157       36422  profile  'bruce', 'faddegon', 'phd', 'ucsf', 'helen', '...\n",
       "25974       39212  profile  'mit', 'nse', 'faculti', 'neil', 'todrea''kepc...\n",
       "35900       53763   others  'exrna', 'home''advertis', 'exrna', 'accept', ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df = pd.read_csv(data_dir+'train_with_tokens_no_html.csv',usecols=[\"Webpage_id\",\"Tag\",\"title_tokens\",\"body_tokens\"])\n",
    "token_df['all_tokens'] =  token_df['title_tokens']+token_df['body_tokens']\n",
    "token_df['all_tokens'] = token_df['all_tokens'].str.replace('[','').str.replace(']','').replace('\"','')\n",
    "token_df.drop(['title_tokens','body_tokens'],axis=1,inplace=True)\n",
    "token_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Webpage_id</th>\n",
       "      <th>domain_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22384</th>\n",
       "      <td>33792</td>\n",
       "      <td>pacificcancercar, com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16995</th>\n",
       "      <td>25409</td>\n",
       "      <td>fitnessandpow, com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53364</th>\n",
       "      <td>79242</td>\n",
       "      <td>mit, edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10588</th>\n",
       "      <td>15603</td>\n",
       "      <td>acr, org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35298</th>\n",
       "      <td>52862</td>\n",
       "      <td>bmj, com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Webpage_id        domain_features\n",
       "22384       33792  pacificcancercar, com\n",
       "16995       25409     fitnessandpow, com\n",
       "53364       79242               mit, edu\n",
       "10588       15603               acr, org\n",
       "35298       52862               bmj, com"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_df = pd.read_csv(data_dir+'train.csv',usecols=['Webpage_id','Domain'])\n",
    "domain_df['domain_main'] = domain_df['Domain'].apply(lambda x: tldextract.extract(x).domain)\n",
    "domain_df['domain_suffix'] = domain_df['Domain'].apply(lambda x: tldextract.extract(x).suffix)\n",
    "domain_df['domain_features'] = (domain_df['domain_main']+\",\"+domain_df['domain_suffix']).apply(nltkPipe)\n",
    "domain_df.drop(['Domain','domain_main','domain_suffix'],axis=1,inplace=True)\n",
    "domain_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53447, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Webpage_id</th>\n",
       "      <th>Tag</th>\n",
       "      <th>all_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3523</th>\n",
       "      <td>5144</td>\n",
       "      <td>profile</td>\n",
       "      <td>'dr', 'john', 'capurro', 'md', 'milford', 'oh'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44944</th>\n",
       "      <td>67043</td>\n",
       "      <td>profile</td>\n",
       "      <td>'koji', 'hatsukawa', 'compani', 'inform', 'tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46013</th>\n",
       "      <td>68622</td>\n",
       "      <td>clinicalTrials</td>\n",
       "      <td>'tctr', 'thai', 'clinic', 'trial', 'registri',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24466</th>\n",
       "      <td>36801</td>\n",
       "      <td>profile</td>\n",
       "      <td>'societi', 'intervent', 'radiolog', 'sir', 'do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30944</th>\n",
       "      <td>46147</td>\n",
       "      <td>forum</td>\n",
       "      <td>'mysteri', 'infertilityhonesti''ten', 'minut',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Webpage_id             Tag  \\\n",
       "3523         5144         profile   \n",
       "44944       67043         profile   \n",
       "46013       68622  clinicalTrials   \n",
       "24466       36801         profile   \n",
       "30944       46147           forum   \n",
       "\n",
       "                                              all_tokens  \n",
       "3523   'dr', 'john', 'capurro', 'md', 'milford', 'oh'...  \n",
       "44944  'koji', 'hatsukawa', 'compani', 'inform', 'tak...  \n",
       "46013  'tctr', 'thai', 'clinic', 'trial', 'registri',...  \n",
       "24466  'societi', 'intervent', 'radiolog', 'sir', 'do...  \n",
       "30944  'mysteri', 'infertilityhonesti''ten', 'minut',...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_df = pd.merge(token_df, domain_df, how='inner', on='Webpage_id')\n",
    "print(source_df.shape)\n",
    "source_df['all_tokens'] = source_df['all_tokens']+', '+source_df['domain_features']\n",
    "source_df.drop('domain_features',axis=1,inplace=True)\n",
    "source_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input='content', analyzer='word')\n",
    "svd = TruncatedSVD(n_components=500, n_iter=5, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_array = vectorizer.fit_transform(source_df['all_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_array = svd.fit_transform(word_vector_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_features = vectorizer.get_feature_names()\n",
    "eigen_features = [vector_features[i] for i in svd.components_[0].argsort()[::-1]][:500]\n",
    "\n",
    "train_df = pd.DataFrame(svd_array,columns=eigen_features)\n",
    "train_df['Tag'] = source_df['Tag']\n",
    "    \n",
    "tags = train_df['Tag'].unique().tolist()\n",
    "tags.sort()\n",
    "\n",
    "tag_dict = {key: value for (key, value) in zip(tags,range(len(tags)))}\n",
    "\n",
    "train_df['Tag_encoded'] = train_df['Tag'].map(tag_dict)\n",
    "train_df = train_df.drop('Tag',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>cancer</th>\n",
       "      <th>health</th>\n",
       "      <th>bayer</th>\n",
       "      <th>research</th>\n",
       "      <th>product</th>\n",
       "      <th>inform</th>\n",
       "      <th>use</th>\n",
       "      <th>gsk</th>\n",
       "      <th>develop</th>\n",
       "      <th>...</th>\n",
       "      <th>graduat</th>\n",
       "      <th>throughout</th>\n",
       "      <th>whether</th>\n",
       "      <th>among</th>\n",
       "      <th>total</th>\n",
       "      <th>sustain</th>\n",
       "      <th>mobil</th>\n",
       "      <th>forum</th>\n",
       "      <th>centr</th>\n",
       "      <th>Tag_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45644</th>\n",
       "      <td>0.148401</td>\n",
       "      <td>-0.084348</td>\n",
       "      <td>-0.038424</td>\n",
       "      <td>-0.042225</td>\n",
       "      <td>-0.029757</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>-0.028058</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>-0.027872</td>\n",
       "      <td>0.028766</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015298</td>\n",
       "      <td>0.033753</td>\n",
       "      <td>-0.001467</td>\n",
       "      <td>-0.012642</td>\n",
       "      <td>-0.012301</td>\n",
       "      <td>-0.014438</td>\n",
       "      <td>-0.047051</td>\n",
       "      <td>-0.009287</td>\n",
       "      <td>-0.021176</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50755</th>\n",
       "      <td>0.154386</td>\n",
       "      <td>-0.032234</td>\n",
       "      <td>-0.014983</td>\n",
       "      <td>-0.005945</td>\n",
       "      <td>-0.002034</td>\n",
       "      <td>-0.005360</td>\n",
       "      <td>-0.006871</td>\n",
       "      <td>-0.015065</td>\n",
       "      <td>0.037669</td>\n",
       "      <td>0.016059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011327</td>\n",
       "      <td>-0.015481</td>\n",
       "      <td>-0.001123</td>\n",
       "      <td>-0.000761</td>\n",
       "      <td>-0.002274</td>\n",
       "      <td>0.011933</td>\n",
       "      <td>-0.004356</td>\n",
       "      <td>0.015494</td>\n",
       "      <td>-0.007348</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6991</th>\n",
       "      <td>0.131928</td>\n",
       "      <td>-0.026124</td>\n",
       "      <td>-0.020698</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>-0.013720</td>\n",
       "      <td>0.006033</td>\n",
       "      <td>-0.028934</td>\n",
       "      <td>-0.000562</td>\n",
       "      <td>0.005782</td>\n",
       "      <td>0.013431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035397</td>\n",
       "      <td>-0.007723</td>\n",
       "      <td>-0.053035</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>-0.013196</td>\n",
       "      <td>-0.030810</td>\n",
       "      <td>0.031708</td>\n",
       "      <td>0.037635</td>\n",
       "      <td>-0.001763</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30828</th>\n",
       "      <td>0.065833</td>\n",
       "      <td>-0.008795</td>\n",
       "      <td>-0.007658</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>-0.017303</td>\n",
       "      <td>-0.008882</td>\n",
       "      <td>0.056970</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.011923</td>\n",
       "      <td>0.045095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006847</td>\n",
       "      <td>-0.002478</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>-0.007234</td>\n",
       "      <td>-0.015462</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>-0.003010</td>\n",
       "      <td>-0.006490</td>\n",
       "      <td>-0.005172</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19752</th>\n",
       "      <td>0.109169</td>\n",
       "      <td>0.016723</td>\n",
       "      <td>-0.007267</td>\n",
       "      <td>0.027117</td>\n",
       "      <td>-0.004144</td>\n",
       "      <td>-0.003396</td>\n",
       "      <td>0.038657</td>\n",
       "      <td>0.023709</td>\n",
       "      <td>0.005463</td>\n",
       "      <td>0.045329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007441</td>\n",
       "      <td>-0.002396</td>\n",
       "      <td>-0.017509</td>\n",
       "      <td>0.009909</td>\n",
       "      <td>0.027611</td>\n",
       "      <td>0.013205</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.020046</td>\n",
       "      <td>-0.026557</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49379</th>\n",
       "      <td>0.055385</td>\n",
       "      <td>-0.036873</td>\n",
       "      <td>-0.026656</td>\n",
       "      <td>-0.023732</td>\n",
       "      <td>-0.009926</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>-0.015163</td>\n",
       "      <td>0.021394</td>\n",
       "      <td>-0.020639</td>\n",
       "      <td>-0.009577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006707</td>\n",
       "      <td>-0.011304</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.015450</td>\n",
       "      <td>-0.001336</td>\n",
       "      <td>0.006149</td>\n",
       "      <td>0.015433</td>\n",
       "      <td>0.009121</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46275</th>\n",
       "      <td>0.055629</td>\n",
       "      <td>-0.085488</td>\n",
       "      <td>0.460310</td>\n",
       "      <td>-0.166245</td>\n",
       "      <td>0.007595</td>\n",
       "      <td>-0.034809</td>\n",
       "      <td>0.080248</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>-0.253839</td>\n",
       "      <td>-0.088894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>-0.000194</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>-0.000471</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36571</th>\n",
       "      <td>0.125965</td>\n",
       "      <td>-0.046169</td>\n",
       "      <td>-0.018657</td>\n",
       "      <td>-0.003270</td>\n",
       "      <td>-0.024569</td>\n",
       "      <td>-0.007449</td>\n",
       "      <td>0.009288</td>\n",
       "      <td>0.008520</td>\n",
       "      <td>-0.024646</td>\n",
       "      <td>0.044337</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023248</td>\n",
       "      <td>-0.021361</td>\n",
       "      <td>0.007438</td>\n",
       "      <td>-0.014241</td>\n",
       "      <td>-0.020298</td>\n",
       "      <td>-0.008134</td>\n",
       "      <td>-0.002147</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>-0.006846</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>0.036869</td>\n",
       "      <td>-0.008067</td>\n",
       "      <td>-0.019351</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>-0.001342</td>\n",
       "      <td>0.034133</td>\n",
       "      <td>0.081261</td>\n",
       "      <td>0.010176</td>\n",
       "      <td>-0.043716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.010042</td>\n",
       "      <td>0.020556</td>\n",
       "      <td>-0.017408</td>\n",
       "      <td>-0.004907</td>\n",
       "      <td>0.007852</td>\n",
       "      <td>0.008303</td>\n",
       "      <td>-0.010102</td>\n",
       "      <td>-0.004037</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30820</th>\n",
       "      <td>0.143354</td>\n",
       "      <td>-0.061969</td>\n",
       "      <td>-0.064200</td>\n",
       "      <td>-0.002835</td>\n",
       "      <td>-0.057456</td>\n",
       "      <td>-0.027892</td>\n",
       "      <td>0.075034</td>\n",
       "      <td>-0.038988</td>\n",
       "      <td>-0.055222</td>\n",
       "      <td>-0.003986</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043678</td>\n",
       "      <td>-0.002637</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.038268</td>\n",
       "      <td>0.035648</td>\n",
       "      <td>-0.006908</td>\n",
       "      <td>0.004277</td>\n",
       "      <td>-0.000765</td>\n",
       "      <td>0.013599</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        patient    cancer    health     bayer  research   product    inform  \\\n",
       "45644  0.148401 -0.084348 -0.038424 -0.042225 -0.029757  0.004467 -0.028058   \n",
       "50755  0.154386 -0.032234 -0.014983 -0.005945 -0.002034 -0.005360 -0.006871   \n",
       "6991   0.131928 -0.026124 -0.020698  0.004053 -0.013720  0.006033 -0.028934   \n",
       "30828  0.065833 -0.008795 -0.007658  0.039200 -0.017303 -0.008882  0.056970   \n",
       "19752  0.109169  0.016723 -0.007267  0.027117 -0.004144 -0.003396  0.038657   \n",
       "49379  0.055385 -0.036873 -0.026656 -0.023732 -0.009926  0.003048 -0.015163   \n",
       "46275  0.055629 -0.085488  0.460310 -0.166245  0.007595 -0.034809  0.080248   \n",
       "36571  0.125965 -0.046169 -0.018657 -0.003270 -0.024569 -0.007449  0.009288   \n",
       "868    0.036869 -0.008067 -0.019351  0.000896  0.003995 -0.001342  0.034133   \n",
       "30820  0.143354 -0.061969 -0.064200 -0.002835 -0.057456 -0.027892  0.075034   \n",
       "\n",
       "            use       gsk   develop     ...        graduat  throughout  \\\n",
       "45644  0.005072 -0.027872  0.028766     ...      -0.015298    0.033753   \n",
       "50755 -0.015065  0.037669  0.016059     ...      -0.011327   -0.015481   \n",
       "6991  -0.000562  0.005782  0.013431     ...      -0.035397   -0.007723   \n",
       "30828  0.000565  0.011923  0.045095     ...       0.006847   -0.002478   \n",
       "19752  0.023709  0.005463  0.045329     ...      -0.007441   -0.002396   \n",
       "49379  0.021394 -0.020639 -0.009577     ...       0.006707   -0.011304   \n",
       "46275  0.043347 -0.253839 -0.088894     ...       0.000372   -0.000194   \n",
       "36571  0.008520 -0.024646  0.044337     ...      -0.023248   -0.021361   \n",
       "868    0.081261  0.010176 -0.043716     ...       0.000709    0.010042   \n",
       "30820 -0.038988 -0.055222 -0.003986     ...      -0.043678   -0.002637   \n",
       "\n",
       "        whether     among     total   sustain     mobil     forum     centr  \\\n",
       "45644 -0.001467 -0.012642 -0.012301 -0.014438 -0.047051 -0.009287 -0.021176   \n",
       "50755 -0.001123 -0.000761 -0.002274  0.011933 -0.004356  0.015494 -0.007348   \n",
       "6991  -0.053035 -0.001402 -0.013196 -0.030810  0.031708  0.037635 -0.001763   \n",
       "30828  0.005040 -0.007234 -0.015462  0.002496 -0.003010 -0.006490 -0.005172   \n",
       "19752 -0.017509  0.009909  0.027611  0.013205  0.000551  0.020046 -0.026557   \n",
       "49379 -0.000261  0.005222  0.015450 -0.001336  0.006149  0.015433  0.009121   \n",
       "46275  0.000089 -0.000471 -0.000112 -0.000016  0.000375  0.000448  0.000260   \n",
       "36571  0.007438 -0.014241 -0.020298 -0.008134 -0.002147  0.010233 -0.006846   \n",
       "868    0.020556 -0.017408 -0.004907  0.007852  0.008303 -0.010102 -0.004037   \n",
       "30820  0.003034  0.038268  0.035648 -0.006908  0.004277 -0.000765  0.013599   \n",
       "\n",
       "       Tag_encoded  \n",
       "45644            7  \n",
       "50755            4  \n",
       "6991             3  \n",
       "30828            2  \n",
       "19752            5  \n",
       "49379            7  \n",
       "46275            0  \n",
       "36571            5  \n",
       "868              4  \n",
       "30820            2  \n",
       "\n",
       "[10 rows x 501 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model...\n",
      "building tree 1 of 100building tree 2 of 100\n",
      "\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:   56.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "exported_pipeline = ExtraTreesClassifier(\n",
    "    bootstrap=False, criterion=\"entropy\", max_features=0.7500000000000001,\n",
    "    min_samples_leaf=2, min_samples_split=2, n_estimators=100, verbose=2,\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "x_cols = [x for x in train_df.columns if x != \"Tag_encoded\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df[x_cols],\n",
    "    train_df['Tag_encoded'],\n",
    "    test_size=0.33\n",
    ")\n",
    "print(\"Fitting the model...\")\n",
    "exported_pipeline.fit(X_train, y_train)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "#Sanity qual check\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "preds = exported_pipeline.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 893,    3,    0,    0,    3,    6,    0,   28,    0],\n",
       "       [   0, 1082,    2,    7,   17,  148,   19,  245,    4],\n",
       "       [   0,    1, 1438,    0,   18,    9,    2,    6,    0],\n",
       "       [   0,    6,    1,  374,    4,   40,    0,   18,    0],\n",
       "       [   2,   23,   20,    9, 1857,  604,   15,  119,    0],\n",
       "       [   3,   23,   11,   10,  209, 5431,   59,   68,    5],\n",
       "       [   0,    1,    0,    0,    9,  183, 1511,   19,    1],\n",
       "       [   2,   53,    0,   12,   14,  145,    5, 2274,    0],\n",
       "       [   0,    0,    0,    0,    7,   12,    0,   18,  530]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8707502582796638"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_f1 = f1_score(y_test, preds, average='weighted')\n",
    "model_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, not bad. Lets generate the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tokens...\n",
      "25787  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Done!\n",
      "22618  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Done!\n",
      "19143  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Done!\n",
      "15768  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Done!\n",
      "12325  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Done!\n",
      "9169  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Done!\n",
      "5913  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Done!\n",
      "3023  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Done!\n",
      "0  indices left...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Now turn this pipeline around on the test set.\n",
    "test_source = pd.read_csv(data_dir+\"test.csv\",usecols=['Webpage_id','Domain'])\n",
    "test_df = get_html(test_source, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Webpage_id</th>\n",
       "      <th>all_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>isrctn, develop, valid, caregiv, qualiti, life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>clinic, trial, registclinicaltrialsregist, eu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>clinic, trial, registclinicaltrialsregist, eu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>clinic, trial, registclinicaltrialsregist, eu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>clinic, trial, registclinicaltrialsregist, eu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Webpage_id                                         all_tokens\n",
       "0          31  isrctn, develop, valid, caregiv, qualiti, life...\n",
       "1          32      clinic, trial, registclinicaltrialsregist, eu\n",
       "2          33      clinic, trial, registclinicaltrialsregist, eu\n",
       "3          34      clinic, trial, registclinicaltrialsregist, eu\n",
       "4          35      clinic, trial, registclinicaltrialsregist, eu"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_df.to_csv(data_dir+\"test_tokens.csv\",index=False)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def listWrapper(row):\n",
    "#     None\n",
    "\n",
    "# def fillEmptyTokens(frame):\n",
    "#     empty_indices = frame[frame.iloc[:]['all_tokens'].str.len()==0].index.values.tolist()\n",
    "#     frame.loc[empty_indices,'all_tokens'] = [[['the']] for x in empty_indices]\n",
    "#     return frame\n",
    "\n",
    "# fillEmptyTokens(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector_array = test_df['all_tokens'].apply(lambda x: vectorizer.transform(x.split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector_arr = vectorizer.transform(test_df['all_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_svd_array = svd.transform(test_vector_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>cancer</th>\n",
       "      <th>health</th>\n",
       "      <th>bayer</th>\n",
       "      <th>research</th>\n",
       "      <th>product</th>\n",
       "      <th>inform</th>\n",
       "      <th>use</th>\n",
       "      <th>gsk</th>\n",
       "      <th>develop</th>\n",
       "      <th>...</th>\n",
       "      <th>produc</th>\n",
       "      <th>graduat</th>\n",
       "      <th>throughout</th>\n",
       "      <th>whether</th>\n",
       "      <th>among</th>\n",
       "      <th>total</th>\n",
       "      <th>sustain</th>\n",
       "      <th>mobil</th>\n",
       "      <th>forum</th>\n",
       "      <th>centr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.173647</td>\n",
       "      <td>-0.049837</td>\n",
       "      <td>0.008551</td>\n",
       "      <td>-0.012303</td>\n",
       "      <td>-0.021304</td>\n",
       "      <td>-0.006715</td>\n",
       "      <td>0.006104</td>\n",
       "      <td>-0.023268</td>\n",
       "      <td>0.028254</td>\n",
       "      <td>0.011883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002451</td>\n",
       "      <td>-0.006524</td>\n",
       "      <td>0.037639</td>\n",
       "      <td>0.014929</td>\n",
       "      <td>-0.012422</td>\n",
       "      <td>0.011763</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>-0.001192</td>\n",
       "      <td>0.015627</td>\n",
       "      <td>-0.012872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.089805</td>\n",
       "      <td>-0.084448</td>\n",
       "      <td>0.308336</td>\n",
       "      <td>-0.110524</td>\n",
       "      <td>0.026413</td>\n",
       "      <td>0.092157</td>\n",
       "      <td>0.060744</td>\n",
       "      <td>-0.029284</td>\n",
       "      <td>-0.071050</td>\n",
       "      <td>-0.051477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019758</td>\n",
       "      <td>-0.034769</td>\n",
       "      <td>-0.026266</td>\n",
       "      <td>-0.001027</td>\n",
       "      <td>0.006043</td>\n",
       "      <td>-0.016143</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>-0.053213</td>\n",
       "      <td>0.001117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.089805</td>\n",
       "      <td>-0.084448</td>\n",
       "      <td>0.308336</td>\n",
       "      <td>-0.110524</td>\n",
       "      <td>0.026413</td>\n",
       "      <td>0.092157</td>\n",
       "      <td>0.060744</td>\n",
       "      <td>-0.029284</td>\n",
       "      <td>-0.071050</td>\n",
       "      <td>-0.051477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019758</td>\n",
       "      <td>-0.034769</td>\n",
       "      <td>-0.026266</td>\n",
       "      <td>-0.001027</td>\n",
       "      <td>0.006043</td>\n",
       "      <td>-0.016143</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>-0.053213</td>\n",
       "      <td>0.001117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.089805</td>\n",
       "      <td>-0.084448</td>\n",
       "      <td>0.308336</td>\n",
       "      <td>-0.110524</td>\n",
       "      <td>0.026413</td>\n",
       "      <td>0.092157</td>\n",
       "      <td>0.060744</td>\n",
       "      <td>-0.029284</td>\n",
       "      <td>-0.071050</td>\n",
       "      <td>-0.051477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019758</td>\n",
       "      <td>-0.034769</td>\n",
       "      <td>-0.026266</td>\n",
       "      <td>-0.001027</td>\n",
       "      <td>0.006043</td>\n",
       "      <td>-0.016143</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>-0.053213</td>\n",
       "      <td>0.001117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.089805</td>\n",
       "      <td>-0.084448</td>\n",
       "      <td>0.308336</td>\n",
       "      <td>-0.110524</td>\n",
       "      <td>0.026413</td>\n",
       "      <td>0.092157</td>\n",
       "      <td>0.060744</td>\n",
       "      <td>-0.029284</td>\n",
       "      <td>-0.071050</td>\n",
       "      <td>-0.051477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019758</td>\n",
       "      <td>-0.034769</td>\n",
       "      <td>-0.026266</td>\n",
       "      <td>-0.001027</td>\n",
       "      <td>0.006043</td>\n",
       "      <td>-0.016143</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>-0.053213</td>\n",
       "      <td>0.001117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    patient    cancer    health     bayer  research   product    inform  \\\n",
       "0  0.173647 -0.049837  0.008551 -0.012303 -0.021304 -0.006715  0.006104   \n",
       "1  0.089805 -0.084448  0.308336 -0.110524  0.026413  0.092157  0.060744   \n",
       "2  0.089805 -0.084448  0.308336 -0.110524  0.026413  0.092157  0.060744   \n",
       "3  0.089805 -0.084448  0.308336 -0.110524  0.026413  0.092157  0.060744   \n",
       "4  0.089805 -0.084448  0.308336 -0.110524  0.026413  0.092157  0.060744   \n",
       "\n",
       "        use       gsk   develop    ...       produc   graduat  throughout  \\\n",
       "0 -0.023268  0.028254  0.011883    ...    -0.002451 -0.006524    0.037639   \n",
       "1 -0.029284 -0.071050 -0.051477    ...    -0.019758 -0.034769   -0.026266   \n",
       "2 -0.029284 -0.071050 -0.051477    ...    -0.019758 -0.034769   -0.026266   \n",
       "3 -0.029284 -0.071050 -0.051477    ...    -0.019758 -0.034769   -0.026266   \n",
       "4 -0.029284 -0.071050 -0.051477    ...    -0.019758 -0.034769   -0.026266   \n",
       "\n",
       "    whether     among     total   sustain     mobil     forum     centr  \n",
       "0  0.014929 -0.012422  0.011763  0.001554 -0.001192  0.015627 -0.012872  \n",
       "1 -0.001027  0.006043 -0.016143  0.010707  0.009766 -0.053213  0.001117  \n",
       "2 -0.001027  0.006043 -0.016143  0.010707  0.009766 -0.053213  0.001117  \n",
       "3 -0.001027  0.006043 -0.016143  0.010707  0.009766 -0.053213  0.001117  \n",
       "4 -0.001027  0.006043 -0.016143  0.010707  0.009766 -0.053213  0.001117  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_predict = pd.DataFrame(test_svd_array,columns=eigen_features)\n",
    "test_df_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "submission_preds = exported_pipeline.predict(test_df_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the inverted tag map for mapping back\n",
    "tag_dict_inverted = {v: k for k, v in tag_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Webpage_id</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>clinicalTrials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Webpage_id             Tag\n",
       "0          31  clinicalTrials\n",
       "1          32          others\n",
       "2          33          others\n",
       "3          34          others\n",
       "4          35          others"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame(data=submission_preds,columns=['Tag'],index=test_df_predict.index)\n",
    "submission_df['Tag'] = submission_df['Tag'].map(tag_dict_inverted)\n",
    "submission_df['Webpage_id'] = test_df['Webpage_id'].values.tolist()\n",
    "col_order = [\"Webpage_id\",\"Tag\"]\n",
    "submission_df = submission_df[col_order]\n",
    "submission_df.to_csv('submission_01.csv',index=False)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
