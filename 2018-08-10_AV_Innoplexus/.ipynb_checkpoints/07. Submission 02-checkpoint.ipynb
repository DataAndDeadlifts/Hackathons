{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to do to improve performance.\n",
    "\n",
    "So the first submission didn't score as well as the train set, not surprising!\n",
    "\n",
    "Lets try and;\n",
    "\n",
    "1. Extract more of the text from the html, the body and title aren't enough it seems.\n",
    "2. Over/undersample and see if I improve performance with either strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jdber\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jdber\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tldextract\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "data_dir = \"../data/2018-08-10_AV_Innoplexus/\"\n",
    "\n",
    "#After we use get_text, use nltk's clean_html function.\n",
    "def nltkPipe(soup_text):\n",
    "    #Convert to tokens\n",
    "    tokens = [x.lower() for x in wordpunct_tokenize(soup_text)]\n",
    "    text = nltk.Text(tokens)\n",
    "    #Get lowercase words. No single letters, and no stop words\n",
    "    words = [w.lower() for w in text if w.isalpha() and len(w) > 1 and w.lower() not in stop_words]\n",
    "    #Remove prefix/suffixes to cut down on vocab\n",
    "    stemmer = EnglishStemmer()\n",
    "    words_nostems = [stemmer.stem(w) for w in words]\n",
    "    return ', '.join(words_nostems)\n",
    "\n",
    "def getTitleTokens(soup):\n",
    "    soup_title = soup.title\n",
    "    if soup_title != None:\n",
    "        soup_title_text = soup.title.get_text()\n",
    "        text_arr = nltkPipe(soup_title_text)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def getBodyTokens(soup):\n",
    "    #Get the text body\n",
    "    soup_para = soup.find_all('p')\n",
    "    if soup_para != None:\n",
    "        soup_para_clean = ' '.join([x.get_text() for x in soup_para if x.span==None and x.a==None])\n",
    "        text_arr = nltkPipe(soup_para_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def getDomainTokens(domainstr):\n",
    "    domain_extracted = tldextract.extract(domainstr)#.domain\n",
    "    domain_tokens = nltkPipe(domain_extracted.domain+\",\"+domain_extracted.suffix)\n",
    "    return domain_tokens\n",
    "\n",
    "def getUrlTokens(url):\n",
    "    domain_split = url.rsplit('/')\n",
    "    if len(domain_split) > 1:\n",
    "        domain_split_elements = ' '.join(domain_split[1:])\n",
    "        domain_split_tokens = nltkPipe(domain_split_elements)\n",
    "        return domain_split_tokens\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def getDescriptionTokens(soup):\n",
    "    #Get the text body\n",
    "    soup_desc = soup.find_all('dl')\n",
    "    if soup_desc != None:\n",
    "        soup_desc_clean = ' '.join([x.get_text() for x in soup_desc])\n",
    "        text_arr = nltkPipe(soup_desc_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def getHeaderTokens(soup):\n",
    "    #Get the html header tokens\n",
    "    soup_heads = soup.find_all('header')\n",
    "    if soup_heads != None:\n",
    "        soup_heads_clean = ' '.join([x.get_text() for x in soup_heads])\n",
    "        text_arr = nltkPipe(soup_heads_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def getHeadTokens(soup):\n",
    "    #Get the html head tokens\n",
    "    soup_head = soup.find_all('head')\n",
    "    if soup_head != None:\n",
    "        soup_head_clean = ' '.join([x.get_text() for x in soup_head])\n",
    "        text_arr = nltkPipe(soup_head_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def getFontTokens(soup):\n",
    "    soup_font = soup.find_all('font')\n",
    "    if soup_font != None:\n",
    "        soup_font_clean = ' '.join([x.get_text() for x in soup_font])\n",
    "        text_arr = nltkPipe(soup_font_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def getTableTokens(soup):\n",
    "    soup_table = soup.find_all('table')\n",
    "    soup_table_headers = [[a for a in x.find_all('th')] for x in soup_table]\n",
    "    soup_table_cells = [[a for a in x.find_all('td')] for x in soup_table]\n",
    "    if soup_table != None:\n",
    "        soup_table_headers_clean = ' '.join([' '.join([a.get_text() for a in x]) for x in soup_table_headers])\n",
    "        soup_table_cells_clean = ' '.join([' '.join([a.get_text() for a in x]) for x in soup_table_cells])\n",
    "        text_arr = nltkPipe(soup_table_headers_clean+soup_table_cells_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def getHrefTokens(soup):\n",
    "    soup_href = soup.find_all('href')\n",
    "    if soup_href != None:\n",
    "        soup_href_clean = ' '.join([x.get_text() for x in soup_href])\n",
    "        text_arr = nltkPipe(soup_href_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def getListTokens(soup):\n",
    "    soup_list = soup.find_all('ol') + soup.find_all('ul')\n",
    "    if soup_list != None:\n",
    "        soup_list_items = [x.find_all('li') for x in soup_list]\n",
    "        soup_list_items_clean = ' '.join([' '.join([a.get_text() for a in x]) for x in soup_list_items])\n",
    "        text_arr = nltkPipe(soup_list_items_clean)\n",
    "        return text_arr\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def get_all_tokens(frame):\n",
    "    print(\"Parsing domain tokens...\")\n",
    "    domain_tokens = frame['Domain'].apply(getDomainTokens)\n",
    "    print(\"Parsing url tokens...\")\n",
    "    url_tokens = frame['Url'].apply(getUrlTokens)\n",
    "    print(\"Parsing soup...\")\n",
    "    soup = frame['Html'].apply(lambda x: BeautifulSoup(x, 'html.parser'))\n",
    "    print(\"Getting title tokens...\")\n",
    "    title_tokens = soup.apply(getTitleTokens)\n",
    "    print(\"Getting body tokens...\")\n",
    "    body_tokens = soup.apply(getBodyTokens)\n",
    "    print(\"Getting description tokens...\")\n",
    "    description_tokens = soup.apply(getDescriptionTokens)\n",
    "    print(\"Getting header tokens...\")\n",
    "    header_tokens = soup.apply(getHeaderTokens)\n",
    "    print(\"Getting head-metadata tokens...\")\n",
    "    head_tokens = soup.apply(getHeadTokens)\n",
    "    print(\"Getting font tokens...\")\n",
    "    font_tokens = soup.apply(getFontTokens)\n",
    "    print(\"Getting table tokens...\")\n",
    "    table_tokens = soup.apply(getTableTokens)\n",
    "    print(\"Getting href tokens...\")\n",
    "    href_tokens = soup.apply(getHrefTokens)\n",
    "    print(\"Getting list tokens...\")\n",
    "    list_tokens = soup.apply(getListTokens)\n",
    "    print(\"Done!\")\n",
    "    return title_tokens + body_tokens + domain_tokens + url_tokens\\\n",
    "    + description_tokens + header_tokens + head_tokens + font_tokens +\\\n",
    "    table_tokens + href_tokens + list_tokens\n",
    "\n",
    "#Build the model\n",
    "def get_html(in_df, out_file_name, chunk_size=5000, overwrite=False, test=False):\n",
    "    keep_cols = [\"Webpage_id\",\"Tag\",\"Url\",\"Domain\"]\n",
    "    if test:\n",
    "        keep_cols = [\"Webpage_id\",\"Domain\",\"Url\"]\n",
    "    if os.path.isfile(data_dir+out_file_name)==False:\n",
    "        if test:\n",
    "            out_frame = pd.DataFrame(columns=[\"Webpage_id\",\"all_tokens\",\"Tag\"])\n",
    "        else:\n",
    "            out_frame = pd.DataFrame(columns=[\"Webpage_id\",\"all_tokens\"])\n",
    "        out_frame.to_csv(data_dir+out_file_name,index=False)\n",
    "    else:\n",
    "        if overwrite:\n",
    "            if test:\n",
    "                out_frame = pd.DataFrame(columns=[\"Webpage_id\",\"all_tokens\",\"Tag\"])\n",
    "            else:\n",
    "                out_frame = pd.DataFrame(columns=[\"Webpage_id\",\"all_tokens\"])\n",
    "            out_frame.to_csv(data_dir+out_file_name,index=False)\n",
    "    use_df = in_df[keep_cols]\n",
    "    html_reader_obj = pd.read_csv(data_dir+'html_data.csv',iterator=True, chunksize=chunk_size)\n",
    "    match_indices = use_df['Webpage_id'].values.tolist()\n",
    "    print(\"Getting tokens...\")\n",
    "    print(len(match_indices),' indices left...')\n",
    "    while len(match_indices) > 0:\n",
    "        for chunk in html_reader_obj:\n",
    "            merge_df = pd.merge(use_df,chunk,how='inner',on='Webpage_id')\n",
    "            merge_df['all_tokens'] = get_all_tokens(merge_df)\n",
    "            merge_df.drop(['Html','Domain','Url'],axis=1,inplace=True)\n",
    "            merge_indices = merge_df['Webpage_id'].values.tolist()\n",
    "            match_indices = [x for x in match_indices if x not in merge_indices]\n",
    "            print(len(match_indices),' indices left...')\n",
    "            concat_frame = pd.read_csv(data_dir+out_file_name)\n",
    "            return_frame = pd.concat([concat_frame,merge_df],ignore_index=True)\n",
    "            return_frame.to_csv(data_dir+out_file_name,index=False)\n",
    "            #frames.append(merge_df)\n",
    "    #Process HTMl for bags of words of the body and title.\n",
    "    #process_df = pd.concat(frames)\n",
    "    print(\"Done! You can get your file at\\n\"+data_dir+out_file_name)\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"Return the estimator and the object to transform the test data.\"\"\"    \n",
    "    train_df = pd.read_csv(data_dir+'train.csv')\n",
    "    tags = train_df['Tag']\n",
    "    #Get tokens\n",
    "    train_df = get_html(train_df)\n",
    "    #Fit_transform to tdfif matrix\n",
    "    print(\"Transforming to tdfif_matrix...\")\n",
    "    train_df = vectorizer.fit_transform(train_df['all_tokens'])\n",
    "    #Prune unneeded features\n",
    "    print(\"Performing SVD...\")\n",
    "    train_df = svd.fit_transform(train_df)\n",
    "    \n",
    "    vector_features = vectorizer.get_feature_names()\n",
    "    eigen_features = [vector_features[i] for i in svd.components_[0].argsort()[::-1]][:500]\n",
    "\n",
    "    train_df = pd.DataFrame(train_df,columns=eigen_features)\n",
    "    train_df['Tag'] = tags\n",
    "    \n",
    "    tags = train_df['Tag'].unique().tolist()\n",
    "    tags.sort()\n",
    "\n",
    "    tag_dict = {key: value for (key, value) in zip(tags,range(len(tags)))}\n",
    "\n",
    "    train_df['Tag_encoded'] = train_df['Tag'].map(tag_dict)\n",
    "    train_df = train_df.drop('Tag',axis=1)\n",
    "    #Build the model\n",
    "    print(\"Building the model...\")\n",
    "    exported_pipeline = make_pipeline(\n",
    "        StackingEstimator(\n",
    "            estimator=ExtraTreesClassifier(\n",
    "                bootstrap=False, criterion=\"gini\", max_features=0.2, \n",
    "                min_samples_leaf=11, min_samples_split=17, n_estimators=100)\n",
    "        ),\n",
    "        ExtraTreesClassifier(\n",
    "            bootstrap=False, criterion=\"entropy\", max_features=0.5, \n",
    "            min_samples_leaf=6, min_samples_split=9, n_estimators=100\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    x_cols = [x for x in train_df_svd.columns if x != \"Tag_encoded\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        train_df[x_cols],\n",
    "        train_df['Tag_encoded'],\n",
    "        test_size=0.33\n",
    "    )\n",
    "    print(\"Fitting the model...\")\n",
    "    exported_pipeline.fit(X_train, y_train)\n",
    "    print(\"Done!\")\n",
    "    return exported_pipeline, vectorizer, svd, tag_dict\n",
    "\n",
    "def prep_test(vectorizer_obj, svd_obj):\n",
    "    \"\"\"Transform test dataset for predicting.\"\"\"\n",
    "    print(\"Getting tokens from html...\")\n",
    "    test_df = pd.read_csv(data_dir+'test.csv')\n",
    "    #Get the HTMl\n",
    "    test_df_tokens = get_html(test_df)\n",
    "    #Transform to tdfif matrix\n",
    "    print(\"Transforming to tfidf matrix...\")\n",
    "    test_df_tdif = vectorizer_obj.transform(test_df_tokens['all_tokens'])\n",
    "    #Prune unneeded features\n",
    "    print(\"Performing SVD...\")\n",
    "    test_svd_array = svd_obj.transform(test_df_tdif)\n",
    "    \n",
    "    vector_features = vectorizer_obj.get_feature_names()\n",
    "    eigen_features = [vector_features[i] for i in svd_obj.components_[0].argsort()[::-1]][:500]\n",
    "    #Map to dataframe\n",
    "    test_df_svd = pd.DataFrame(test_svd_array,columns=eigen_features)\n",
    "    test_df_svd['Tag'] = test_df['Tag']\n",
    "    print(\"Done!\")\n",
    "    return test_df_svd\n",
    "\n",
    "def main():\n",
    "    #Get the model\n",
    "    print(\"Getting the model, transform objects and tag-dict...\")\n",
    "    model, vectorizer_obj, svd_obj, tag_dict = build_model()\n",
    "    #Prep the test set\n",
    "    print(\"Prepping the test dataset...\")\n",
    "    test_df = prep_test(vectorizer_obj, svd_obj)\n",
    "    print(\"Making predictions...\")\n",
    "    predictions = model.predict(test_df)\n",
    "    print(\"Formatting predictions...\")\n",
    "    print(\"Saving predictions for submission...\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try and extract more tokens. I want to extract tokens from; description, headings, highlights, special fonts, table and list elements.\n",
    "\n",
    "To do this I'll write an html reading script for a subset of rows to test my parser on. Then I'll incorporate into the get_html function and make sure I get all tokens to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Webpage_id</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Url</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7693</th>\n",
       "      <td>11525</td>\n",
       "      <td>www.endocrine-abstracts.org</td>\n",
       "      <td>http://www.endocrine-abstracts.org/ea/0048/ea0...</td>\n",
       "      <td>conferences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12843</th>\n",
       "      <td>19271</td>\n",
       "      <td>ijbnpa.biomedcentral.com</td>\n",
       "      <td>https://ijbnpa.biomedcentral.com/articles/10.1...</td>\n",
       "      <td>publication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>7627</td>\n",
       "      <td>ecommons.cornell.edu</td>\n",
       "      <td>https://ecommons.cornell.edu/handle/1813/11103</td>\n",
       "      <td>thesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48379</th>\n",
       "      <td>71636</td>\n",
       "      <td>www.mdsabstracts.com</td>\n",
       "      <td>http://www.mdsabstracts.com/abstract.asp?Meeti...</td>\n",
       "      <td>conferences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43419</th>\n",
       "      <td>64940</td>\n",
       "      <td>www.amgen.com</td>\n",
       "      <td>http://www.amgen.com/responsibility/access-to-...</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Webpage_id                       Domain  \\\n",
       "7693        11525  www.endocrine-abstracts.org   \n",
       "12843       19271     ijbnpa.biomedcentral.com   \n",
       "5215         7627         ecommons.cornell.edu   \n",
       "48379       71636         www.mdsabstracts.com   \n",
       "43419       64940                www.amgen.com   \n",
       "\n",
       "                                                     Url          Tag  \n",
       "7693   http://www.endocrine-abstracts.org/ea/0048/ea0...  conferences  \n",
       "12843  https://ijbnpa.biomedcentral.com/articles/10.1...  publication  \n",
       "5215      https://ecommons.cornell.edu/handle/1813/11103       thesis  \n",
       "48379  http://www.mdsabstracts.com/abstract.asp?Meeti...  conferences  \n",
       "43419  http://www.amgen.com/responsibility/access-to-...       others  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(data_dir+'train.csv')\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Webpage_id</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23430</th>\n",
       "      <td>72968</td>\n",
       "      <td>adc.bmj.com</td>\n",
       "      <td>http://adc.bmj.com/content/91/6/469.long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8769</th>\n",
       "      <td>26472</td>\n",
       "      <td>www.nidirect.gov.uk</td>\n",
       "      <td>https://www.nidirect.gov.uk/news/buildings-and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17842</th>\n",
       "      <td>53985</td>\n",
       "      <td>safetyinhealth.biomedcentral.com</td>\n",
       "      <td>https://safetyinhealth.biomedcentral.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>8926</td>\n",
       "      <td>emergingmaterials.conferenceseries.com</td>\n",
       "      <td>http://emergingmaterials.conferenceseries.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21581</th>\n",
       "      <td>65631</td>\n",
       "      <td>www.lilly.com</td>\n",
       "      <td>https://www.lilly.com/who-we-are/business-area...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Webpage_id                                  Domain  \\\n",
       "23430       72968                             adc.bmj.com   \n",
       "8769        26472                     www.nidirect.gov.uk   \n",
       "17842       53985        safetyinhealth.biomedcentral.com   \n",
       "2805         8926  emergingmaterials.conferenceseries.com   \n",
       "21581       65631                           www.lilly.com   \n",
       "\n",
       "                                                     Url  \n",
       "23430           http://adc.bmj.com/content/91/6/469.long  \n",
       "8769   https://www.nidirect.gov.uk/news/buildings-and...  \n",
       "17842           https://safetyinhealth.biomedcentral.com  \n",
       "2805       http://emergingmaterials.conferenceseries.com  \n",
       "21581  https://www.lilly.com/who-we-are/business-area...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(data_dir+'test.csv')\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tokens...\n",
      "53447  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "50065  indices left...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdber\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:199: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n",
      "Getting title tokens...\n",
      "Getting body tokens...\n",
      "Getting description tokens...\n",
      "Getting header tokens...\n",
      "Getting head-metadata tokens...\n",
      "Getting font tokens...\n",
      "Getting table tokens...\n",
      "Getting href tokens...\n",
      "Getting list tokens...\n",
      "Done!\n",
      "46616  indices left...\n",
      "Parsing domain tokens...\n",
      "Parsing url tokens...\n",
      "Parsing soup...\n"
     ]
    }
   ],
   "source": [
    "get_html(train_df,'train_df_all_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
